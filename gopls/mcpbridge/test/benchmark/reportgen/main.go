// Command to generate a markdown report from benchmark_results.json
// Usage: go run reportgen/main.go [results.json]

package main

import (
	"encoding/json"
	"flag"
	"fmt"
	"os"
	"strings"
	"time"
)

// BenchmarkResult represents a single benchmark result from JSON.
type BenchmarkResult struct {
	Name            string  `json:"Name"`
	Category        string  `json:"Category"`
	Duration        int64   `json:"Duration"` // nanoseconds
	MinDuration     int64   `json:"MinDuration"`
	MaxDuration     int64   `json:"MaxDuration"`
	StdDev          int64   `json:"StdDev"`
	Iterations      int     `json:"Iterations"`
	Success         bool    `json:"Success"`
	Error           string  `json:"Error"`
	ItemsFound      int     `json:"ItemsFound"`
	BytesProcessed  int     `json:"BytesProcessed"`
	ComparisonNote  string  `json:"ComparisonNote"`
	SpeedupFactor   float64 `json:"SpeedupFactor"`
	TraditionalMean int64   `json:"TraditionalMean"`
	TraditionalMin  int64   `json:"TraditionalMin"`
	TraditionalMax  int64   `json:"TraditionalMax"`
}

// BenchmarkSummary provides aggregated statistics.
type BenchmarkSummary struct {
	TotalBenchmarks int    `json:"total_benchmarks"`
	Successful      int    `json:"successful"`
	Failed          int    `json:"failed"`
	AverageDuration int64  `json:"average_duration"` // nanoseconds
	TotalItemsFound int    `json:"total_items_found"`
	SpeedupRange    string `json:"speedup_range"`
}

// BenchmarkSuite represents a complete benchmark run.
type BenchmarkSuite struct {
	Timestamp     int64             `json:"timestamp"`
	GoVersion     string            `json:"go_version"`
	OS            string            `json:"os"`
	Arch          string            `json:"arch"`
	TotalDuration int64             `json:"total_duration"` // nanoseconds
	Results       []BenchmarkResult `json:"results"`
	Summary       BenchmarkSummary  `json:"summary"`
	ProjectInfo   ProjectInfo       `json:"project_info"`
}

// ProjectInfo contains statistics about the project.
type ProjectInfo struct {
	Name     string `json:"name"`
	Path     string `json:"path"`
	Packages int    `json:"packages"`
	Files    int    `json:"files"`
	LOC      int    `json:"lines_of_code"`
}

func main() {
	compact := flag.Bool("compact", false, "Generate compact report for PR comments")
	flag.Parse()

	args := flag.Args()
	jsonFile := "benchmark_results.json"
	if len(args) > 0 {
		jsonFile = args[0]
	}

	// Read JSON file
	data, err := os.ReadFile(jsonFile)
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error reading %s: %v\n", jsonFile, err)
		os.Exit(1)
	}

	var suite BenchmarkSuite
	if err := json.Unmarshal(data, &suite); err != nil {
		fmt.Fprintf(os.Stderr, "Error parsing JSON: %v\n", err)
		os.Exit(1)
	}

	// Generate markdown report
	report := generateReport(suite, *compact)

	// Write to RESULTS.md (or stdout if preferred, but writing to file is safer for now)
	outputFile := "RESULTS.md"
	if *compact {
		outputFile = "RESULTS_COMPACT.md"
	}

	if err := os.WriteFile(outputFile, []byte(report), 0644); err != nil {
		fmt.Fprintf(os.Stderr, "Error writing %s: %v\n", outputFile, err)
		os.Exit(1)
	}

	fmt.Printf("âœ… Generated %s from %s (compact=%v)\n", outputFile, jsonFile, *compact)
}

func generateReport(suite BenchmarkSuite, compact bool) string {
	var sb strings.Builder

	// Header
	sb.WriteString("# ðŸš€ gopls-mcp Benchmark Results\n\n")

	if !compact {
		sb.WriteString("âš ï¸ **AUTO-GENERATED** - This file is automatically generated from `benchmark_results.json`\n\n")
		sb.WriteString("**DO NOT EDIT MANUALLY** - Changes will be overwritten on next benchmark run.\n\n")
		sb.WriteString(fmt.Sprintf("**Generated:** %s\n\n", time.Unix(suite.Timestamp, 0).Format("2006-01-02 15:04:05")))
		sb.WriteString(fmt.Sprintf("**Go Version:** %s\n\n", suite.GoVersion))
		sb.WriteString(fmt.Sprintf("**Platform:** %s/%s\n\n", suite.OS, suite.Arch))

		// Table of Contents
		sb.WriteString("## Table of Contents\n\n")
		sb.WriteString("- [Test Environment](#test-environment)\n")
		sb.WriteString("- [Summary](#summary)\n")
		sb.WriteString("- [Comparison Benchmarks](#comparison-benchmarks)\n")
		sb.WriteString("- [All Benchmarks](#all-benchmarks)\n")
		sb.WriteString("- [Performance by Category](#performance-by-category)\n")
		sb.WriteString("- [Variance Analysis](#variance-analysis)\n")
		sb.WriteString("\n---\n\n")

		// Test Environment
		sb.WriteString("## Test Environment\n\n")
		sb.WriteString("### Project Information\n\n")
		sb.WriteString("| Metric | Value |\n")
		sb.WriteString("|--------|-------|\n")
		sb.WriteString(fmt.Sprintf("| Project | %s |\n", suite.ProjectInfo.Name))
		sb.WriteString(fmt.Sprintf("| Path | %s |\n", suite.ProjectInfo.Path))
		sb.WriteString(fmt.Sprintf("| Packages | %d |\n", suite.ProjectInfo.Packages))
		sb.WriteString(fmt.Sprintf("| Files | %d |\n", suite.ProjectInfo.Files))
		sb.WriteString(fmt.Sprintf("| Lines of Code | %d |\n", suite.ProjectInfo.LOC))
		sb.WriteString("\n")
	}

	// Summary
	if compact {
		sb.WriteString("### ðŸ“Š Summary\n\n")
	} else {
		sb.WriteString("## Summary\n\n")
		sb.WriteString("### Overall Statistics\n\n")
	}

	// Table format for both modes
	sb.WriteString("| Metric | Value |\n")
	sb.WriteString("|--------|-------|\n")
	sb.WriteString(fmt.Sprintf("| Total Duration | %s |\n", formatDuration(suite.TotalDuration)))
	sb.WriteString(fmt.Sprintf("| Benchmarks | %d total, %d successful, %d failed |\n",
		suite.Summary.TotalBenchmarks, suite.Summary.Successful, suite.Summary.Failed))
	sb.WriteString(fmt.Sprintf("| Average Latency | %s |\n", formatDuration(suite.Summary.AverageDuration)))
	sb.WriteString(fmt.Sprintf("| Speedup Range | %s |\n", suite.Summary.SpeedupRange))
	if !compact {
		sb.WriteString(fmt.Sprintf("| Total Items Found | %d |\n", suite.Summary.TotalItemsFound))
	}
	sb.WriteString("\n")

	// Comparison Benchmarks
	if compact {
		sb.WriteString("### âš¡ Comparison vs CLI\n\n")
	} else {
		sb.WriteString("## Comparison Benchmarks\n\n")
		sb.WriteString("These benchmarks measure **steady-state speedup** after warm cache is achieved.\n")
		sb.WriteString("**Note:** Cold start overhead (server startup) is measured separately. See README.md \"Cold Start Analysis\" section.\n\n")
	}

	comparisons := filterByCategory(suite.Results, "Comparison")
	if len(comparisons) > 0 {
		if compact {
			// Simplified table for compact mode
			sb.WriteString("| Benchmark | MCP | Traditional | Speedup |\n")
			sb.WriteString("|-----------|-----|-------------|---------|\n")
			for _, r := range comparisons {
				name := strings.Replace(r.Name, "Comparison: ", "", 1)
				sb.WriteString(fmt.Sprintf("| **%s** | %s | %s | **%.1fx** |\n",
					name,
					formatDuration(r.Duration),
					formatDuration(r.TraditionalMean),
					r.SpeedupFactor,
				))
			}
		} else {
			// Full table for full mode
			sb.WriteString("| Benchmark | Iterations | MCP Mean | Min | Max | StdDev | CV | Traditional | Steady-State Speedup |\n")
			sb.WriteString("|-----------|------------|----------|-----|-----|--------|-----|-------------|----------------------|\n")

			for _, r := range comparisons {
				cv := calculateCV(r.StdDev, r.Duration)
				sb.WriteString(fmt.Sprintf("| **%s** | %d | %s | %s | %s | Â±%s | %.1f%% | %s | **%.1fx** |\n",
					strings.Replace(r.Name, "Comparison: ", "", 1),
					r.Iterations,
					formatDuration(r.Duration),
					formatDuration(r.MinDuration),
					formatDuration(r.MaxDuration),
					formatDuration(r.StdDev),
					cv,
					formatDuration(r.TraditionalMean),
					r.SpeedupFactor,
				))
			}
		}
		sb.WriteString("\n")

		// Explanation for grep performance
		if compact {
			sb.WriteString("**Note:** `grep -r` is faster for simple text matching, but semantic tools provide:\n")
			sb.WriteString("- âœ… Type-safe navigation (no false positives like \"Handler\" matching \"handler\")\n")
			sb.WriteString("- âœ… Fuzzy matching (find symbols with partial names)\n")
			sb.WriteString("- âœ… Cross-reference analysis (find all usages, including through interfaces)\n")
			sb.WriteString("- âœ… Call hierarchy and semantic understanding\n")
			sb.WriteString("\n")
		}

		// Detailed comparison results (Full mode only)
		if !compact {
			sb.WriteString("### Detailed Results\n\n")
			for _, r := range comparisons {
				benchmarkName := strings.Replace(r.Name, "Comparison: ", "", 1)
				sb.WriteString(fmt.Sprintf("#### %s\n\n", benchmarkName))
				sb.WriteString(fmt.Sprintf("- **Iterations:** %d\n", r.Iterations))
				sb.WriteString(fmt.Sprintf("- **MCP Mean:** %s (Â±%s)\n", formatDuration(r.Duration), formatDuration(r.StdDev)))
				sb.WriteString(fmt.Sprintf("- **Range:** %s - %s\n", formatDuration(r.MinDuration), formatDuration(r.MaxDuration)))
				sb.WriteString(fmt.Sprintf("- **Traditional:** %s\n", formatDuration(r.TraditionalMean)))
				sb.WriteString(fmt.Sprintf("- **Steady-State Speedup:** %.1fx faster\n", r.SpeedupFactor))
				sb.WriteString(fmt.Sprintf("- **Note:** Speedup measured after warm cache. Cold start adds ~1.2s overhead (see README.md).\n"))

				if benchmarkName == "go build" {
					sb.WriteString(fmt.Sprintf("- **Why This Comparison:** While `go build` performs full compilation and\n"))
					sb.WriteString(fmt.Sprintf("  `go_build_check` provides incremental type-checking, both serve the same\n"))
					sb.WriteString(fmt.Sprintf("  user intent: **'check for errors after changes'**.\n"))
					sb.WriteString(fmt.Sprintf("  go_build_check provides the instant feedback loop developers expect\n"))
					sb.WriteString(fmt.Sprintf("  in their editor, making this the relevant comparison for interactive workflows.\n"))
				}

				cv := calculateCV(r.StdDev, r.Duration)
				sb.WriteString(fmt.Sprintf("- **Variance (CV):** %.1f%% - %s\n", cv, getReliabilityLabel(cv)))
				if r.ItemsFound > 0 {
					sb.WriteString(fmt.Sprintf("- **Items Found:** %d\n", r.ItemsFound))
				}
				sb.WriteString("\n")
			}
		}
	}

	// Performance by Category
	if compact {
		sb.WriteString("### ðŸ“ˆ Performance by Category\n\n")
	} else {
		sb.WriteString("## Performance by Category\n\n")
	}

	categories := groupByCategory(suite.Results)
	sb.WriteString("| Category | Average Latency | Tests |\n")
	sb.WriteString("|----------|----------------|-------|\n")
	for cat, results := range categories {
		var total int64
		for _, r := range results {
			total += r.Duration
		}
		avg := total / int64(len(results))
		sb.WriteString(fmt.Sprintf("| %s | %s | %d |\n", cat, formatDuration(avg), len(results)))
	}
	sb.WriteString("\n")

	// Full report sections (Full mode only)
	if !compact {
		sb.WriteString("## All Benchmarks\n\n")
		sb.WriteString("**Note:** Speedup factors shown are **steady-state** (warm cache). Cold start overhead is measured separately.\n\n")
		sb.WriteString("| Category | Benchmark | Duration | Iterations | Steady-State Speedup |\n")
		sb.WriteString("|----------|-----------|----------|------------|----------------------|\n")

		for _, r := range suite.Results {
			iterations := "1"
			if r.Iterations > 0 {
				iterations = fmt.Sprintf("%d", r.Iterations)
			}
			speedup := fmt.Sprintf("%.1fx", r.SpeedupFactor)
			if r.SpeedupFactor == 0 {
				speedup = "N/A"
			}
			sb.WriteString(fmt.Sprintf("| %s | %s | %s | %s | %s |\n",
				r.Category, r.Name, formatDuration(r.Duration), iterations, speedup))
		}
		sb.WriteString("\n")

		sb.WriteString("## Variance Analysis\n\n")
		sb.WriteString("### Coefficient of Variation (CV)\n\n")
		sb.WriteString("CV = (StdDev / Mean) Ã— 100%\n\n")
		sb.WriteString("| CV Range | Interpretation |\n")
		sb.WriteString("|-----------|----------------|\n")
		sb.WriteString("| < 10% | âœ… Very consistent, reliable |\n")
		sb.WriteString("| 10-25% | âš ï¸ Moderate variance, acceptable |\n")
		sb.WriteString("| > 25% | âŒ High variance, results less reliable |\n")
		sb.WriteString("\n")

		hasComparisons := false
		for _, r := range suite.Results {
			if r.Iterations > 0 && r.StdDev > 0 {
				if !hasComparisons {
					sb.WriteString("### Variance by Comparison Benchmark\n\n")
					sb.WriteString("| Benchmark | Iterations | StdDev | CV | Reliability |\n")
					sb.WriteString("|-----------|------------|--------|-----|-------------|\n")
					hasComparisons = true
				}
				cv := calculateCV(r.StdDev, r.Duration)
				sb.WriteString(fmt.Sprintf("| %s | %d | Â±%s | %.1f%% | %s |\n",
					strings.Replace(r.Name, "Comparison: ", "", 1),
					r.Iterations,
					formatDuration(r.StdDev),
					cv,
					getReliabilityLabel(cv),
				))
			}
		}
		if hasComparisons {
			sb.WriteString("\n")
		}
	}

	// Footer
	if !compact {
		sb.WriteString("---\n\n")
		sb.WriteString("## How to Regenerate This Report\n\n")
		sb.WriteString("```bash\n")
		sb.WriteString("cd gopls/mcpbridge/test/benchmark\n")
		sb.WriteString("# Run benchmarks\n")
		sb.WriteString("go run benchmark_main.go -compare\n")
		sb.WriteString("# Generate report from JSON\n")
		sb.WriteString("go run reportgen/main.go benchmark_results.json\n")
		sb.WriteString("```\n\n")
		sb.WriteString(fmt.Sprintf("**Note:** This report was generated from `benchmark_results.json` on %s\n",
			time.Unix(suite.Timestamp, 0).Format("2006-01-02 15:04:05")))
	}

	return sb.String()
}

func formatDuration(ns int64) string {
	if ns == 0 {
		return "N/A"
	}

	d := time.Duration(ns)
	switch {
	case d < time.Microsecond:
		return fmt.Sprintf("%dns", ns)
	case d < time.Millisecond:
		return fmt.Sprintf("%.1fÂµs", float64(ns)/float64(time.Microsecond))
	case d < time.Second:
		return fmt.Sprintf("%.2fms", float64(ns)/float64(time.Millisecond))
	case d < time.Minute:
		return fmt.Sprintf("%.2fs", float64(ns)/float64(time.Second))
	default:
		minutes := int(d.Minutes())
		seconds := float64(d%time.Minute) / float64(time.Second)
		return fmt.Sprintf("%dm%.1fs", minutes, seconds)
	}
}

func calculateCV(stdDev, mean int64) float64 {
	if mean == 0 {
		return 0
	}
	return (float64(stdDev) / float64(mean)) * 100
}

func getReliabilityLabel(cv float64) string {
	switch {
	case cv < 10:
		return "âœ… Very consistent"
	case cv < 25:
		return "âš ï¸ Moderate variance"
	default:
		return "âŒ High variance"
	}
}

func filterByCategory(results []BenchmarkResult, category string) []BenchmarkResult {
	var filtered []BenchmarkResult
	for _, r := range results {
		if r.Category == category {
			filtered = append(filtered, r)
		}
	}
	return filtered
}

func groupByCategory(results []BenchmarkResult) map[string][]BenchmarkResult {
	grouped := make(map[string][]BenchmarkResult)
	for _, r := range results {
		grouped[r.Category] = append(grouped[r.Category], r)
	}
	return grouped
}
